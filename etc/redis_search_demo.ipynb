{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f64918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import redis\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.exceptions import ResponseError\n",
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path().absolute() / '..' / 'dev-config.json', 'r') as f:\n",
    "    CONFIG = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, index_name, *, config):\n",
    "        self._config = config\n",
    "        self._index_name = index_name\n",
    "        \n",
    "        self._open_ai_emb = OpenAIEmbeddings(openai_api_key=self._config['openai_api_key'])\n",
    "        self._redis = redis.Redis(host='localhost', port=6379, db=0)\n",
    "        \n",
    "        self._init_redis()\n",
    "        \n",
    "    def _init_redis(self):\n",
    "        try:\n",
    "            self._redis.ft(self._index_name).dropindex()\n",
    "        except ResponseError:\n",
    "            pass\n",
    "        \n",
    "        emb_field = VectorField(\n",
    "            name='embedding',\n",
    "            algorithm='HNSW',\n",
    "            attributes=dict(\n",
    "                type='FLOAT64',\n",
    "                dim=1536,\n",
    "                distance_metric='COSINE',\n",
    "            ),\n",
    "        )\n",
    "        version_field = TextField('version')\n",
    "        \n",
    "        self._redis.ft(self._index_name).create_index([\n",
    "            emb_field,\n",
    "            version_field,\n",
    "        ])\n",
    "\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.array:\n",
    "        embedding = self._open_ai_emb.embed_query(text)\n",
    "        \n",
    "        return np.array(embedding)\n",
    "    \n",
    "    def _save_embedding(self, text: str, embedding: np.array) -> bytes:\n",
    "        self._redis.hset(f\"text:{text}\", mapping = dict(\n",
    "            embedding=embedding.tobytes(),\n",
    "            text=text,\n",
    "            version=VERSION,\n",
    "        ))\n",
    "                \n",
    "    def add(self, text: str) -> np.array:\n",
    "        found = self._redis.hget(f'text:{text}', 'version')\n",
    "        if found and found == VERSION:\n",
    "            return found\n",
    "        \n",
    "        embedding = self._get_embedding(text)\n",
    "        self._save_embedding(text, embedding)\n",
    "\n",
    "        return embedding.tobytes()\n",
    "\n",
    "    def knn(self, text: str, *, k: int = 100) -> list:\n",
    "        q = (\n",
    "            Query(f\"(@version:{VERSION})=>[KNN {k} @embedding $e]\")\n",
    "            .return_field('text')\n",
    "            .return_field('__embedding_score')\n",
    "            .dialect(2)\n",
    "        )\n",
    "        result = self._redis.ft(self._index_name).search(q, query_params={\"e\": self._get_embedding(text).tobytes()})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def close(self):\n",
    "        self._s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57749910",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embeddings('examplegpt', config=CONFIG)\n",
    "e.add('Пользователя зовут Вадим')\n",
    "e.add('Жену пользователя зовут Катя')\n",
    "e.add('У пользователя и у жены одинаковые фамилии')\n",
    "e.add('Фамилия пользователя — Пуштаев')\n",
    "e.add('Пользователь женат')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    MORE = 'MORE_INFO_NEEDED'\n",
    "    \n",
    "    def __init__(self, *, config, embeddings):\n",
    "        self._config = config\n",
    "        self._embeddings = embeddings\n",
    "        \n",
    "        self._chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=config['openai_api_key'], max_tokens=1200, model_kwargs={'temperature': 0.3})\n",
    "        self._prompt = dedent(\n",
    "            f\"\"\"\n",
    "            You are a helpful assistant.\n",
    "            If you need more details or context ask me using the following format: `{self.MORE}: QUERY`, where QUERY is a query to knowledge DB based on embeddings.\n",
    "            I'll execute the query and give you some facts to work with.\n",
    "            Never use `{self.MORE}` if it's not a query.\n",
    "            Never use the same query twice, always come up with something new. Remeber that we can do it more than once to find an answer.\n",
    "            ---\n",
    "            \"\"\".strip() + '\\n'\n",
    "        )\n",
    "        self._history = []\n",
    "        \n",
    "    def __call__(self, text) -> str:\n",
    "        return self.say(text)\n",
    "    \n",
    "    def say(self, text, depth=0) -> str:\n",
    "        self._history.append(HumanMessage(content=self._prompt + text))\n",
    "        response = self._chat(self._history)\n",
    "        self._history.append(response)\n",
    "        response_text = response.content\n",
    "        \n",
    "        if response_text.startswith(self.MORE) and depth < 3:\n",
    "            request = response_text[len(self.MORE) + 2:]\n",
    "            facts = []\n",
    "            for e in self._embeddings.knn(request, k=2).docs:\n",
    "                self._history.append(HumanMessage(content=f'Please consider the following to be a fact: {e.text}'))\n",
    "            return self.say(text, depth=depth+1)\n",
    "\n",
    "        self._print_dialog()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _print_dialog(self):\n",
    "        print('---')\n",
    "        for x in self._history:\n",
    "            print(type(x))\n",
    "            print(x.content)\n",
    "            print('')\n",
    "        print('---')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(config=CONFIG, embeddings=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c139e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat('Как зовут жену Вадима Пуштаева?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdab31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
